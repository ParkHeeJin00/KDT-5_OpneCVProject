{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:02:51.273490Z",
     "start_time": "2024-03-27T06:02:41.421308Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\anaconda3\\envs\\Pytorch38\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 지정된 프로시저를 찾을 수 없습니다'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "import torchmetrics.functional as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#PreProcessing\n",
    "preprocessing = transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:02:51.289081Z",
     "start_time": "2024-03-27T06:02:51.279319Z"
    }
   },
   "id": "1c9edb87147b8bb6",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#폴뎌명이 분류 클래스가 되는 데이터셋 만들기\n",
    "trainroot = 'datas/mid_dogs/'\n",
    "dataset = ImageFolder(root=trainroot, transform=preprocessing) #전처리\n",
    "loader = DataLoader(dataset=dataset, batch_size=10, shuffle=True) #데이터로더"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:02:51.676945Z",
     "start_time": "2024-03-27T06:02:51.292402Z"
    }
   },
   "id": "bfe7d9dd754a08aa",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 데이터셋을 트레인셋과 테스트셋으로 나눔\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "test_dataset = Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "# 트레인셋과 테스트셋에 대한 DataLoader 각각 생성\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:02:51.692411Z",
     "start_time": "2024-03-27T06:02:51.679933Z"
    }
   },
   "id": "9977f930651e8a4c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_classes = len(dataset.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:02:51.707353Z",
     "start_time": "2024-03-27T06:02:51.697382Z"
    }
   },
   "id": "f85d66d8247e2ebf",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 이미지 / 라벨 분리\n",
    "images, labels = [], []\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:02:51.723016Z",
     "start_time": "2024-03-27T06:02:51.711337Z"
    }
   },
   "id": "9962291d094e583e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for img, lab in dataset:\n",
    "    images.append(np.array(img))\n",
    "    labels.append(lab)\n",
    "\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:18.223284Z",
     "start_time": "2024-03-27T06:02:51.727004Z"
    }
   },
   "id": "4e8a8ef25dff3666",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn.functional as f"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:18.245406Z",
     "start_time": "2024-03-27T06:03:18.225462Z"
    }
   },
   "id": "10a3648c6ea8aa81",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class mid_dogs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #합성곱 계층 정의하기\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        \n",
    "        #전결합 계층 정의하기\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # 모델의 출력 크기를 클래스 수에 맞게 조정\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 합성곱 계층 연산\n",
    "        x = f.relu(self.conv1(x))\n",
    "        x = f.max_pool2d(x, 2, 2)  # 2x2 max pooling\n",
    "        x = f.relu(self.conv2(x))\n",
    "        x = f.max_pool2d(x, 2, 2)\n",
    "        x = f.relu(self.conv3(x))\n",
    "        x = f.max_pool2d(x, 2, 2)\n",
    "    \n",
    "        # 전결합 계층 연산\n",
    "        x = x.view(-1, 64 * 4 * 4)  # batch size 유지하면서 flatten\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:18.268941Z",
     "start_time": "2024-03-27T06:03:18.249052Z"
    }
   },
   "id": "aab1dc5c04d07418",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#인스턴스 생성\n",
    "model = mid_dogs()\n",
    "\n",
    "#DEVICE\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 손실함수\n",
    "C = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#에포크Z\n",
    "EPOCHS =100\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:19.187465Z",
     "start_time": "2024-03-27T06:03:18.270934Z"
    }
   },
   "id": "c5f5e5572e777f26",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def training(dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs = model(images)\n",
    "        labels = labels.long()\n",
    "        loss = C(outputs, labels)\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    return running_loss/len(dataloader) # 평균 손실 반환"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:19.202998Z",
     "start_time": "2024-03-27T06:03:19.189458Z"
    }
   },
   "id": "74da1bc5f0a24f33",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "3.39344587548059"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:45.691141Z",
     "start_time": "2024-03-27T06:03:19.204981Z"
    }
   },
   "id": "845ca234d2ee3bbd",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def testing(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for cnt, (feature, target) in enumerate(dataloader):\n",
    "            feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            pre_target = model(feature)\n",
    "            \n",
    "            loss = C(pre_target, target.long())\n",
    "            val_loss.append(loss)\n",
    "            \n",
    "        acc = metrics.accuracy(pre_target, target, task='multiclass', num_classes=num_classes)\n",
    "        print(f'[VALID_LOSS] = {loss}\\n\\n[ACCURACY] = {acc.item()}')\n",
    "        return val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:45.707089Z",
     "start_time": "2024-03-27T06:03:45.693134Z"
    }
   },
   "id": "5d7183571d264414",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID_LOSS] = 10.961725234985352\n",
      "\n",
      "[ACCURACY] = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "[tensor(4.5734),\n tensor(4.6045),\n tensor(4.3933),\n tensor(4.5033),\n tensor(4.5374),\n tensor(4.3778),\n tensor(4.4140),\n tensor(4.4689),\n tensor(5.4514),\n tensor(11.2467),\n tensor(13.0104),\n tensor(11.6972),\n tensor(12.1407),\n tensor(12.8456),\n tensor(11.4671),\n tensor(12.8446),\n tensor(11.2664),\n tensor(13.0564),\n tensor(13.4563),\n tensor(13.5229),\n tensor(12.1563),\n tensor(11.4499),\n tensor(13.1345),\n tensor(12.5368),\n tensor(10.9902),\n tensor(12.3911),\n tensor(11.1229),\n tensor(12.0682),\n tensor(10.9107),\n tensor(10.6646),\n tensor(10.8259),\n tensor(10.4210),\n tensor(10.9867),\n tensor(11.2499),\n tensor(10.3919),\n tensor(11.4286),\n tensor(10.1015),\n tensor(10.4115),\n tensor(10.5127),\n tensor(10.7930),\n tensor(10.3829),\n tensor(11.8688),\n tensor(11.1725),\n tensor(12.4332),\n tensor(11.4826),\n tensor(10.9843),\n tensor(11.1876),\n tensor(10.9562),\n tensor(11.2083),\n tensor(10.8253),\n tensor(11.3523),\n tensor(11.6193),\n tensor(10.8500),\n tensor(12.3253),\n tensor(10.8647),\n tensor(11.2517),\n tensor(10.9312),\n tensor(10.9461),\n tensor(10.1244),\n tensor(9.8998),\n tensor(10.5434),\n tensor(9.7667),\n tensor(10.5915),\n tensor(10.8014),\n tensor(9.8393),\n tensor(10.0956),\n tensor(10.7526),\n tensor(10.4045),\n tensor(10.6487),\n tensor(10.3732),\n tensor(10.3744),\n tensor(10.1977),\n tensor(10.7635),\n tensor(9.9803),\n tensor(10.6082),\n tensor(10.4004),\n tensor(10.9612),\n tensor(9.9873),\n tensor(10.4981),\n tensor(10.5192),\n tensor(10.2817),\n tensor(10.5619),\n tensor(10.6484),\n tensor(9.9765),\n tensor(9.8755),\n tensor(10.5958),\n tensor(10.7941),\n tensor(10.3956),\n tensor(10.2360),\n tensor(10.3459),\n tensor(10.7270),\n tensor(10.8238),\n tensor(11.7006),\n tensor(10.0242),\n tensor(10.4412),\n tensor(10.7867),\n tensor(11.3500),\n tensor(11.0127),\n tensor(11.0328),\n tensor(10.8705),\n tensor(10.8122),\n tensor(10.5251),\n tensor(11.1284),\n tensor(10.4918),\n tensor(11.1878),\n tensor(10.3337),\n tensor(9.9186),\n tensor(11.2834),\n tensor(11.1531),\n tensor(10.3867),\n tensor(9.8419),\n tensor(11.1187),\n tensor(10.6627),\n tensor(10.2961),\n tensor(10.5616),\n tensor(10.0613),\n tensor(10.2230),\n tensor(11.1407),\n tensor(10.9301),\n tensor(10.4077),\n tensor(10.2806),\n tensor(10.8790),\n tensor(10.6757),\n tensor(10.9617)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:50.912544Z",
     "start_time": "2024-03-27T06:03:45.712076Z"
    }
   },
   "id": "77742f4efb7e4f72",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def early_stopping(validation_losses, patience=3):\n",
    "    if len(validation_losses) < patience + 1:\n",
    "        return False  # 최소 횟수의 검증 손실 기록이 없는 경우 훈련 계속\n",
    "    else:\n",
    "        for i in range(1, patience + 1):\n",
    "            if validation_losses[-i] < validation_losses[-(i+1)]:\n",
    "                return False  # 이전 손실보다 현재 손실이 더 작으면 계속 훈련\n",
    "        return True  # 향상되지 않은 횟수가 patience를 초과하면 훈련 중단"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:03:50.928180Z",
     "start_time": "2024-03-27T06:03:50.913678Z"
    }
   },
   "id": "377452c2c90b9101",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID_LOSS] = 12.357385635375977\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 11.733083724975586\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 10.485595703125\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 13.01984977722168\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 18.479141235351562\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 14.626815795898438\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 15.983972549438477\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 16.41154670715332\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 16.89019012451172\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 18.150793075561523\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 17.94178581237793\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 20.06035614013672\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 22.34174346923828\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 23.11543083190918\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 25.658649444580078\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 25.59210777282715\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "[VALID_LOSS] = 31.468544006347656\n",
      "\n",
      "[ACCURACY] = 0.0\n",
      "16th : Early stopping!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(),\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmiddledogs_model.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     17\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m train_loss_avg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_losses\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_losses)\n\u001B[0;32m     20\u001B[0m test_loss_avg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(test_losses)\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(test_losses)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m TRAIN: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss_avg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, VALID: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss_avg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "patience = 3  # 향상되지 않은 횟수\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = training(train_loader)\n",
    "    test_loss = testing(test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    #print(f'[{epoch+1}/{EPOCHS}]\\n\\n TRAIN: {train_loss}, VALID: {test_loss}')\n",
    "    \n",
    "    if epoch >= patience:  # 최소 횟수 이상의 에포크에 대해\n",
    "        validation_losses = test_losses[-(patience + 1):]  # 최근 patience+1 개의 검증 손실을 가져옴\n",
    "        if early_stopping(validation_losses):  # 훈련 중단 조건 충족 여부 확인\n",
    "            print(f\"{epoch}th : Early stopping!\")\n",
    "            torch.save(model.state_dict(),'middledogs_model.pth')\n",
    "            break\n",
    "\n",
    "train_loss_avg = sum(train_losses).item() / len(train_losses)\n",
    "test_loss_avg = sum(test_losses).item() / len(test_losses)\n",
    "print(f'\\n\\n TRAIN: {train_loss_avg}, VALID: {test_loss_avg}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T06:14:22.477287Z",
     "start_time": "2024-03-27T06:03:50.930175Z"
    }
   },
   "id": "b8f085eb557b0fed",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = model\n",
    "model.load_state_dict(torch.load('middledogs_model.pth'))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T06:14:22.484973Z"
    }
   },
   "id": "de4fb97ec67e1560",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "70838970dc1f2ec8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "33c49358779cc643"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
